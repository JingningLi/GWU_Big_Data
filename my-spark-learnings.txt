Through executing codes, I found that It took some time to execute count() and groupBy, which is weird because the count() only needs to count how many lines are loaded from a file into the RDD and the work of groupBy did is not that hard, either.  But after I knew the specific size of that file which has 4898431 lines, I was not surprised because the file indeed is very large. Compared to the experience I have before without using spark, the less than one minute time those two command lines used is very impressive. But as the new comer to Spark, I did expect Spark to be faster. 

Therefore, it inspired me to think about how to save time when dealing with big data using Spark. It is very interesting to see different methods of saving time are used in those notebooks. For example, using sample data to get the approximation of ratio calculated by using the complete dataset. Also, when using the spark sql, adding filters before groupBy took spark less time to run than without adding filters although adding filter means that spark need to execute one more operation. However, adding filters means that less data is piped to the groupBy operation. As a result, the size of data is more critical to decreasing the time spark need to run.

I continued focus on comparing running time and found one weird point. According to the outcome of executing takeSample and Sample, I am surprised by that the one took less time to run is takeSample, which is supposed to be the more time-consuming one because the filter and split were done locally in a single node. 

Through executing codes, I indeed find Spark RDD is designed to be lazy. The RDD will not actually load or  cache data until it runs into the actual action code, which took a lot more time to run. 

Besides, pySpark did have the similar functions that python has, especially in the data frame part. To me, the syntax of pySpark is easier to use and quite clean. Also, the sql in spark is very similar to the normal sql , and it is simpler than the normal sql because different operations in spark sql can be concatenated using only one dot. Also, Sparkâ€™s MLlib is very easy to use and it dose contain some basic machine learning methods like linear regression, decision tree, logistics regression and cluster. Overall, Spark is very easy to use and it is efficient in dealing with big data even though I hope it can become more efficient. 

